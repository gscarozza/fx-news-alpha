{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change working directory to the root of the project\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Add 'src' to Python path\n",
    "sys.path.append(os.path.join(project_root, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Core Libraries --------------------\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Text Preprocessing --------------------\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "# -------------------- Word Embedding --------------------\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import softmax\n",
    "\n",
    "# -------------------- Machine Learning --------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# -------------------- Visualization --------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------- Model Saving --------------------\n",
    "import joblib\n",
    "\n",
    "# -------------------- Pandas Display Options --------------------\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"data/processed/labeled_january_data.csv\"\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)\n",
    "\n",
    "file_path2 = \"data/processed/labeled_february_data.csv\"\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)\n",
    "\n",
    "file_path3 = \"data/processed/labeled_march_data.csv\"\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_march = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "df_jan['cleaned_article'] = df_jan['article'].apply(preprocess_text)\n",
    "df_feb['cleaned_article'] = df_feb['article'].apply(preprocess_text)\n",
    "df_march['cleaned_article'] = df_march['article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Word2Vec approach\n",
    "- custom word2vec model implemented by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized articles\n",
    "tokenized_articles = df_jan['cleaned_article'].apply(str.split).tolist() + df_feb['cleaned_article'].apply(str.split).tolist() \n",
    "# Train Word2Vec\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_articles,\n",
    "    vector_size=300,  # Embedding dimensionality\n",
    "    window=5,         # Context window size\n",
    "    min_count=2,      # Minimum word frequency\n",
    "    sg=1,             # Skip-gram method\n",
    "    workers=4,        # Number of threads\n",
    "    epochs=30         # Number of training iterations\n",
    ")\n",
    "\n",
    "word2vec_model.save(\"models/w2v_embeddings_experiment_v1/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_article(text, model):\n",
    "    \"\"\"\n",
    "    Generate a document vector by averaging word embeddings.\n",
    "    :param text: Preprocessed article as a string.\n",
    "    :param model: Trained Word2Vec model.\n",
    "    :return: Document vector (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Generate embeddings for both datasets\n",
    "df_jan['embedding'] = df_jan['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))\n",
    "df_feb['embedding'] = df_feb['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))\n",
    "df_march['embedding'] = df_march['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words\n",
    "most_frequent_words = word2vec_model.wv.index_to_key[:200]  # Top 200 words\n",
    "word_vectors = np.array([word2vec_model.wv[word] for word in most_frequent_words])\n",
    "\n",
    "# Reduce dimensionality using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "word_embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(word_embeddings_2d[:, 0], word_embeddings_2d[:, 1], alpha=0.7, edgecolors='k')\n",
    "for i, word in enumerate(most_frequent_words):\n",
    "    plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]), fontsize=9)\n",
    "plt.title('t-SNE Visualization of Word Embeddings')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/word2vec_tsne_visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the distribution of the Target variable\n",
    "- helps us to realize a class imbalance\n",
    "- good to keep track of\n",
    "- MOVE THIS INTO the PREPROCESSING file eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_jan)\n",
    "plt.title('Label Distribution in January Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_january.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_feb)\n",
    "plt.title('Label Distribution in February Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_february.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_march)\n",
    "plt.title('Label Distribution in March Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_march.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect class distribution\n",
    "print(\"January class distribution:\")\n",
    "print(df_jan['label'].value_counts())\n",
    "\n",
    "print(\"February class distribution:\")\n",
    "print(df_feb['label'].value_counts())\n",
    "\n",
    "print(\"March class distribution (test set):\")\n",
    "print(df_march['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intra_class_distance(df, class_label):\n",
    "    \n",
    "    class_vectors = np.vstack(df[df['label'] == class_label]['embedding'].values)\n",
    "    if len(class_vectors) <= 1:\n",
    "        return 0  \n",
    "\n",
    "    pairwise_distances = pdist(class_vectors, metric='euclidean')\n",
    "    return np.mean(pairwise_distances)\n",
    "\n",
    "def calculate_inter_class_distance(df, label_1, label_2):\n",
    "\n",
    "    vectors_1 = np.vstack(df[df['label'] == label_1]['embedding'].values)\n",
    "    vectors_2 = np.vstack(df[df['label'] == label_2]['embedding'].values)\n",
    "\n",
    "    pairwise_distances = np.linalg.norm(vectors_1[:, None] - vectors_2, axis=2).flatten()\n",
    "    return np.mean(pairwise_distances)\n",
    "\n",
    "labels = [-1, 0, 1]\n",
    "intra_class_distances = {label: calculate_intra_class_distance(df_jan, label) for label in labels}\n",
    "\n",
    "inter_class_distances = {}\n",
    "for i, label_1 in enumerate(labels):\n",
    "    for label_2 in labels[i + 1:]:\n",
    "        key = f\"{label_1} vs {label_2}\"\n",
    "        inter_class_distances[key] = calculate_inter_class_distance(df_jan, label_1, label_2)\n",
    "\n",
    "print(\"Intra-Class Distances:\")\n",
    "for label, dist in intra_class_distances.items():\n",
    "    print(f\"Label {label}: {dist:.4f}\")\n",
    "\n",
    "print(\"\\nInter-Class Distances:\")\n",
    "for pair, dist in inter_class_distances.items():\n",
    "    print(f\"{pair}: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression Model with Custom Word2Vec Model\n",
    "\n",
    "Task: \n",
    "- Train on January, test on February\n",
    "    - Train / test with 3 classes (+1, 0, -1), yielding a 3 x 3 confusion matrix\n",
    "    - Train / test with 2 classes (+1, -1), yielding a 2 x 2 confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classification\n",
    "# combine df_jan and df_feb for training\n",
    "X_train_multi = np.vstack(np.concatenate((df_jan['embedding'].values, df_feb['embedding'].values)))\n",
    "y_train_multi = np.concatenate((df_jan['label'], df_feb['label']))\n",
    "X_test_multi = np.vstack(df_march['embedding'].values)\n",
    "y_test_multi = df_march['label']\n",
    "\n",
    "# binary classification\n",
    "# filter out rows where label is 0 for binary classification\n",
    "df_train_binary = pd.concat([df_jan, df_feb])\n",
    "df_train_binary = df_train_binary[df_train_binary['label'] != 0]\n",
    "df_march_binary = df_march[df_march['label'] != 0]\n",
    "\n",
    "X_train_binary = np.vstack(df_train_binary['embedding'].values)\n",
    "y_train_binary = df_train_binary['label']\n",
    "X_test_binary = np.vstack(df_march_binary['embedding'].values)\n",
    "y_test_binary = df_march_binary['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Logistic Regression (multi-class)\n",
    "multi_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "multi_model.fit(X_train_multi, y_train_multi)\n",
    "multi_preds = multi_model.predict(X_test_multi)\n",
    "\n",
    "# binary Logistic regression (binary classification)\n",
    "binary_model = LogisticRegression(max_iter=1000)\n",
    "binary_model.fit(X_train_binary, y_train_binary)\n",
    "binary_preds = binary_model.predict(X_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory to save results\n",
    "save_dir = \"results/w2v_embeddings_experiment_v1/metrics\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate multi-class model\n",
    "multi_class_report = classification_report(y_test_multi, multi_preds)\n",
    "multi_class_cm = confusion_matrix(y_test_multi, multi_preds)\n",
    "\n",
    "# Save multi-class metrics\n",
    "with open(os.path.join(save_dir, \"classification_report_multi.txt\"), \"w\") as f:\n",
    "    f.write(\"Multi-class Classification Report:\\n\")\n",
    "    f.write(multi_class_report)\n",
    "\n",
    "# Save multi-class confusion matrix\n",
    "multi_class_cm_file = os.path.join(save_dir, \"confusion_matrix_multi.png\")\n",
    "sns.heatmap(multi_class_cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"-1\", \"0\", \"1\"], yticklabels=[\"-1\", \"0\", \"1\"])\n",
    "plt.title(\"Multi-class Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(multi_class_cm_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate binary model\n",
    "binary_class_report = classification_report(y_test_binary, binary_preds)\n",
    "binary_class_cm = confusion_matrix(y_test_binary, binary_preds)\n",
    "\n",
    "# Save binary metrics\n",
    "with open(os.path.join(save_dir, \"classification_report_binary.txt\"), \"w\") as f:\n",
    "    f.write(\"Binary Classification Report:\\n\")\n",
    "    f.write(binary_class_report)\n",
    "\n",
    "# Save binary confusion matrix\n",
    "binary_class_cm_file = os.path.join(save_dir, \"confusion_matrix_binary.png\")\n",
    "sns.heatmap(binary_class_cm, annot=True, fmt='d', cmap='Greens', xticklabels=[\"-1\", \"1\"], yticklabels=[\"-1\", \"1\"])\n",
    "plt.title(\"Binary Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(binary_class_cm_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(multi_model, 'models/w2v_embeddings_experiment_v1/multi_class_model.pkl')\n",
    "joblib.dump(binary_model, 'models/w2v_embeddings_experiment_v1/binary_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
