{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/january/january.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/february/february.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/march/march.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_march = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    # Replace monetary values (e.g., \"R$ 15 bilhões\" → \"15B\")\n",
    "    text = re.sub(r\"R\\$ ?([\\d.,]+) bilhões\", r\"\\1B\", text)\n",
    "    text = re.sub(r\"R\\$ ?([\\d.,]+) milhões\", r\"\\1M\", text)\n",
    "    # Standardize percentages (e.g., \"0.5 pp\" → \"0.5%\")\n",
    "    text = re.sub(r\"([\\d.,]+) pp\", r\"\\1%\", text)\n",
    "    # Standardize plain numbers (e.g., \"15,000\" → \"15000\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    # Remove dates and times\n",
    "    text = re.sub(r\"\\b\\d{1,2}[hH]\\d{2}\\b\", \"\", text)  # Times like \"17h20\"\n",
    "    text = re.sub(r\"\\b\\d{1,2}ª[Ff]\\b\", \"\", text)       # Ordinals like \"2ªF\"\n",
    "    # Remove redundant words\n",
    "    text = re.sub(r\"MAIS AGENDA|LÁ FORA|A BOLSA ESTÁ CARA\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Comprehensive dictionary for expanding acronyms\n",
    "acronyms = {\n",
    "    \"Selic\": \"Sistema Especial de Liquidação e de Custódia\",\n",
    "    \"PIB\": \"Produto Interno Bruto\",\n",
    "    \"CDI\": \"Certificado de Depósito Interbancário\",\n",
    "    \"LPRs\": \"Loan Prime Rates\",\n",
    "    \"Ibovespa\": \"Índice Bovespa\",\n",
    "    \"BB\": \"Banco do Brasil\",\n",
    "    \"BC\": \"Banco Central\",\n",
    "    \"FGTS\": \"Fundo de Garantia do Tempo de Serviço\",\n",
    "    \"STF\": \"Supremo Tribunal Federal\",\n",
    "    \"CPI\": \"Índice de Preços ao Consumidor\",\n",
    "    \"MP\": \"Medida Provisória\",\n",
    "    \"EUA\": \"Estados Unidos\",\n",
    "    \"ONU\": \"Organização das Nações Unidas\",\n",
    "    \"FGV\": \"Fundação Getúlio Vargas\",\n",
    "    \"IBGE\": \"Instituto Brasileiro de Geografia e Estatística\",\n",
    "    \"BNDES\": \"Banco Nacional de Desenvolvimento Econômico e Social\",\n",
    "    \"IPCA\": \"Índice Nacional de Preços ao Consumidor Amplo\",\n",
    "    \"DI\": \"Depósito Interfinanceiro\",\n",
    "    \"IR\": \"Imposto de Renda\",\n",
    "    \"OI\": \"Operadora Oi\",\n",
    "    \"CV\": \"Câmara de Vereadores\"\n",
    "}\n",
    "\n",
    "noisy_acronyms = {\"ROMI\", \"ENEVA\", \"LIGHT\", \"DA\"}\n",
    "\n",
    "def expand_acronyms(text, acronym_dict):\n",
    "    for acronym, full_form in acronym_dict.items():\n",
    "        text = re.sub(rf'\\b{re.escape(acronym)}\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_noisy_acronyms(text, noisy_set):\n",
    "    return re.sub(r'\\b(?:' + '|'.join(noisy_set) + r')\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Portuguese spaCy model\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = normalize_numbers(text)         \n",
    "    text = expand_acronyms(text, acronyms)\n",
    "    text = remove_noisy_acronyms(text, noisy_acronyms)\n",
    "    text = lemmatize_text(text)          \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df_jan['cleaned_article'] = df_jan['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")\n",
    "df_feb['cleaned_article'] = df_feb['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")\n",
    "df_march['cleaned_article'] = df_march['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Word2Vec approach\n",
    "- custom word2vec model implemented by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare tokenized articles\n",
    "tokenized_articles = df_jan['cleaned_article'].apply(str.split).tolist() + df_feb['cleaned_article'].apply(str.split).tolist() \n",
    "# Train Word2Vec\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_articles,\n",
    "    vector_size=300,  # Embedding dimensionality\n",
    "    window=5,         # Context window size\n",
    "    min_count=2,      # Minimum word frequency\n",
    "    sg=1,             # Skip-gram method\n",
    "    workers=4,        # Number of threads\n",
    "    epochs=30         # Number of training iterations\n",
    ")\n",
    "\n",
    "word2vec_model.save(\"models/w2v_embeddings_experiment_v1/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_article(text, model):\n",
    "    \"\"\"\n",
    "    Generate a document vector by averaging word embeddings.\n",
    "    :param text: Preprocessed article as a string.\n",
    "    :param model: Trained Word2Vec model.\n",
    "    :return: Document vector (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Generate embeddings for both datasets\n",
    "df_jan['embedding'] = df_jan['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))\n",
    "df_feb['embedding'] = df_feb['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))\n",
    "df_march['embedding'] = df_march['cleaned_article'].apply(lambda x: vectorize_article(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get most frequent words\n",
    "most_frequent_words = word2vec_model.wv.index_to_key[:200]  # Top 200 words\n",
    "word_vectors = np.array([word2vec_model.wv[word] for word in most_frequent_words])\n",
    "\n",
    "# Reduce dimensionality using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "word_embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(word_embeddings_2d[:, 0], word_embeddings_2d[:, 1], alpha=0.7, edgecolors='k')\n",
    "for i, word in enumerate(most_frequent_words):\n",
    "    plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]), fontsize=9)\n",
    "plt.title('t-SNE Visualization of Word Embeddings')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/word2vec_tsne_visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the distribution of the Target variable\n",
    "- helps us to realize a class imbalance\n",
    "- good to keep track of\n",
    "- MOVE THIS INTO the PREPROCESSING file eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_jan)\n",
    "plt.title('Label Distribution in January Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_january.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_feb)\n",
    "plt.title('Label Distribution in February Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_february.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_march)\n",
    "plt.title('Label Distribution in March Dataset')\n",
    "plt.savefig(\"results/w2v_embeddings_experiment_v1/figures/label_distribution_march.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect class distribution\n",
    "print(\"January class distribution:\")\n",
    "print(df_jan['label'].value_counts())\n",
    "\n",
    "print(\"February class distribution:\")\n",
    "print(df_feb['label'].value_counts())\n",
    "\n",
    "print(\"March class distribution (test set):\")\n",
    "print(df_march['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_intra_class_distance(df, class_label):\n",
    "    \n",
    "    class_vectors = np.vstack(df[df['label'] == class_label]['embedding'].values)\n",
    "    if len(class_vectors) <= 1:\n",
    "        return 0  \n",
    "\n",
    "    pairwise_distances = pdist(class_vectors, metric='euclidean')\n",
    "    return np.mean(pairwise_distances)\n",
    "\n",
    "def calculate_inter_class_distance(df, label_1, label_2):\n",
    "\n",
    "    vectors_1 = np.vstack(df[df['label'] == label_1]['embedding'].values)\n",
    "    vectors_2 = np.vstack(df[df['label'] == label_2]['embedding'].values)\n",
    "\n",
    "    pairwise_distances = np.linalg.norm(vectors_1[:, None] - vectors_2, axis=2).flatten()\n",
    "    return np.mean(pairwise_distances)\n",
    "\n",
    "labels = [-1, 0, 1]\n",
    "intra_class_distances = {label: calculate_intra_class_distance(df_jan, label) for label in labels}\n",
    "\n",
    "inter_class_distances = {}\n",
    "for i, label_1 in enumerate(labels):\n",
    "    for label_2 in labels[i + 1:]:\n",
    "        key = f\"{label_1} vs {label_2}\"\n",
    "        inter_class_distances[key] = calculate_inter_class_distance(df_jan, label_1, label_2)\n",
    "\n",
    "print(\"Intra-Class Distances:\")\n",
    "for label, dist in intra_class_distances.items():\n",
    "    print(f\"Label {label}: {dist:.4f}\")\n",
    "\n",
    "print(\"\\nInter-Class Distances:\")\n",
    "for pair, dist in inter_class_distances.items():\n",
    "    print(f\"{pair}: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression Model with Custom Word2Vec Model\n",
    "\n",
    "Task: \n",
    "- Train on January, test on February\n",
    "    - Train / test with 3 classes (+1, 0, -1), yielding a 3 x 3 confusion matrix\n",
    "    - Train / test with 2 classes (+1, -1), yielding a 2 x 2 confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classification\n",
    "# combine df_jan and df_feb for training\n",
    "X_train_multi = np.vstack(np.concatenate((df_jan['embedding'].values, df_feb['embedding'].values)))\n",
    "y_train_multi = np.concatenate((df_jan['label'], df_feb['label']))\n",
    "X_test_multi = np.vstack(df_march['embedding'].values)\n",
    "y_test_multi = df_march['label']\n",
    "\n",
    "# binary classification\n",
    "# filter out rows where label is 0 for binary classification\n",
    "df_train_binary = pd.concat([df_jan, df_feb])\n",
    "df_train_binary = df_train_binary[df_train_binary['label'] != 0]\n",
    "df_march_binary = df_march[df_march['label'] != 0]\n",
    "\n",
    "X_train_binary = np.vstack(df_train_binary['embedding'].values)\n",
    "y_train_binary = df_train_binary['label']\n",
    "X_test_binary = np.vstack(df_march_binary['embedding'].values)\n",
    "y_test_binary = df_march_binary['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Logistic Regression (multi-class)\n",
    "multi_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "multi_model.fit(X_train_multi, y_train_multi)\n",
    "multi_preds = multi_model.predict(X_test_multi)\n",
    "\n",
    "# binary Logistic regression (binary classification)\n",
    "binary_model = LogisticRegression(max_iter=1000)\n",
    "binary_model.fit(X_train_binary, y_train_binary)\n",
    "binary_preds = binary_model.predict(X_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Directory to save results\n",
    "save_dir = \"results/w2v_embeddings_experiment_v1/metrics\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate multi-class model\n",
    "multi_class_report = classification_report(y_test_multi, multi_preds)\n",
    "multi_class_cm = confusion_matrix(y_test_multi, multi_preds)\n",
    "\n",
    "# Save multi-class metrics\n",
    "with open(os.path.join(save_dir, \"classification_report_multi.txt\"), \"w\") as f:\n",
    "    f.write(\"Multi-class Classification Report:\\n\")\n",
    "    f.write(multi_class_report)\n",
    "\n",
    "# Save multi-class confusion matrix\n",
    "multi_class_cm_file = os.path.join(save_dir, \"confusion_matrix_multi.png\")\n",
    "sns.heatmap(multi_class_cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"-1\", \"0\", \"1\"], yticklabels=[\"-1\", \"0\", \"1\"])\n",
    "plt.title(\"Multi-class Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(multi_class_cm_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate binary model\n",
    "binary_class_report = classification_report(y_test_binary, binary_preds)\n",
    "binary_class_cm = confusion_matrix(y_test_binary, binary_preds)\n",
    "\n",
    "# Save binary metrics\n",
    "with open(os.path.join(save_dir, \"classification_report_binary.txt\"), \"w\") as f:\n",
    "    f.write(\"Binary Classification Report:\\n\")\n",
    "    f.write(binary_class_report)\n",
    "\n",
    "# Save binary confusion matrix\n",
    "binary_class_cm_file = os.path.join(save_dir, \"confusion_matrix_binary.png\")\n",
    "sns.heatmap(binary_class_cm, annot=True, fmt='d', cmap='Greens', xticklabels=[\"-1\", \"1\"], yticklabels=[\"-1\", \"1\"])\n",
    "plt.title(\"Binary Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(binary_class_cm_file)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(multi_model, 'models/w2v_embeddings_experiment_v1/multi_class_model.pkl')\n",
    "joblib.dump(binary_model, 'models/w2v_embeddings_experiment_v1/binary_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
