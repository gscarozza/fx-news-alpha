{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\fx-news-alpha\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"data/processed/january/january.csv\"\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)\n",
    "\n",
    "file_path2 = \"data/processed/february/february.csv\"\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)\n",
    "\n",
    "file_path3 = \"data/processed/march/march.csv\"\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_march = pd.read_csv(file)\n",
    "\n",
    "df = pd.concat([df_jan, df_feb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTimbau tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    # tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # pass inputs through model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract [CLS] token embedding (shape: [batch_size, hidden_size])\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token is the first token\n",
    "    return cls_embedding.squeeze(0).numpy()  # convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan['embedding'] = df_jan['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "df_feb['embedding'] = df_feb['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "df_march['embedding'] = df_march['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\scaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# NLTK Stop Words\n",
    "nltk.download('stopwords')\n",
    "portuguese_stopwords = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a single TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,  # Adjust as needed\n",
    "    stop_words=portuguese_stopwords,\n",
    "    sublinear_tf=True  # Logarithmic scaling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: 0\n",
      "Processing category: -1\n",
      "Processing category: 1\n",
      "Lexicon saved as domain_specific_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine January and February data for lexicon generation\n",
    "df_train_combined = pd.concat([df_jan, df_feb])\n",
    "\n",
    "# Group articles by their sentiment labels\n",
    "categories = df_train_combined['label'].unique()\n",
    "category_docs = {cat: df_train_combined[df_train_combined['label'] == cat]['cleaned_article'].tolist() for cat in categories}\n",
    "\n",
    "# Generate the lexicon\n",
    "lexicon = {}\n",
    "\n",
    "for category, docs in category_docs.items():\n",
    "    print(f\"Processing category: {category}\")\n",
    "    \n",
    "    # Fit and transform TF-IDF on category-specific articles\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate average TF-IDF score for each term\n",
    "    scores = tfidf_matrix.mean(axis=0).A1\n",
    "    category_lexicon = sorted(\n",
    "        zip(feature_names, scores), key=lambda x: x[1], reverse=True\n",
    "    )[:20]\n",
    "    \n",
    "    # Normalize scores\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_scores = scaler.fit_transform([[s] for _, s in category_lexicon]).flatten()\n",
    "    lexicon[category] = [(term, norm_score) for (term, _), norm_score in zip(category_lexicon, normalized_scores)]\n",
    "\n",
    "# Save the lexicon\n",
    "lexicon_data = [\n",
    "    {'category': category, 'term': term, 'tfidf_score': score}\n",
    "    for category, terms in lexicon.items()\n",
    "    for term, score in terms\n",
    "]\n",
    "lexicon_df = pd.DataFrame(lexicon_data)\n",
    "lexicon_df.to_csv(\"results/tf-idf_experiment_v1/metrics/domain_specific_lexicon.csv\", index=False)\n",
    "print(\"Lexicon saved as domain_specific_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF on January and February data\n",
    "combined_train_text = df_train_combined['cleaned_article']\n",
    "tfidf_vectorizer.fit(combined_train_text)\n",
    "\n",
    "# Transform datasets\n",
    "tfidf_jan = tfidf_vectorizer.transform(df_jan['cleaned_article'])\n",
    "tfidf_feb = tfidf_vectorizer.transform(df_feb['cleaned_article'])\n",
    "tfidf_march = tfidf_vectorizer.transform(df_march['cleaned_article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New January Embedding Shape: (1101, 1268)\n",
      "New February Embedding Shape: (916, 1268)\n",
      "New March Embedding Shape: (713, 1268)\n"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF matrices to dense arrays\n",
    "tfidf_jan_dense = tfidf_jan.toarray()\n",
    "tfidf_feb_dense = tfidf_feb.toarray()\n",
    "tfidf_march_dense = tfidf_march.toarray()\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "embeddings_jan = np.array(df_jan['embedding'].tolist())\n",
    "embeddings_feb = np.array(df_feb['embedding'].tolist())\n",
    "embeddings_march = np.array(df_march['embedding'].tolist())\n",
    "\n",
    "# Combine TF-IDF and embeddings\n",
    "combined_jan = np.hstack((tfidf_jan_dense, embeddings_jan))\n",
    "combined_feb = np.hstack((tfidf_feb_dense, embeddings_feb))\n",
    "combined_march = np.hstack((tfidf_march_dense, embeddings_march))\n",
    "\n",
    "# Update the 'embedding' column\n",
    "df_jan['embedding'] = list(combined_jan)\n",
    "df_feb['embedding'] = list(combined_feb)\n",
    "df_march['embedding'] = list(combined_march)\n",
    "\n",
    "# Verify new embedding shapes\n",
    "print(\"New January Embedding Shape:\", np.array(df_jan['embedding'].tolist()).shape)\n",
    "print(\"New February Embedding Shape:\", np.array(df_feb['embedding'].tolist()).shape)\n",
    "print(\"New March Embedding Shape:\", np.array(df_march['embedding'].tolist()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New January Embedding Shape: (1101, 1768)\n",
      "New February Embedding Shape: (916, 1768)\n",
      "New March Embedding Shape: (713, 1768)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "\n",
    "# Ensure TF-IDF matrices are dense\n",
    "tfidf_jan_dense = tfidf_jan.toarray()\n",
    "tfidf_feb_dense = tfidf_feb.toarray()\n",
    "tfidf_march_dense = tfidf_march.toarray()\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "embeddings_jan = np.array(df_jan['embedding'].tolist())\n",
    "embeddings_feb = np.array(df_feb['embedding'].tolist())\n",
    "embeddings_march = np.array(df_march['embedding'].tolist())\n",
    "\n",
    "# Combine TF-IDF and embeddings\n",
    "combined_jan = np.hstack((tfidf_jan_dense, embeddings_jan))\n",
    "combined_feb = np.hstack((tfidf_feb_dense, embeddings_feb))\n",
    "combined_march = np.hstack((tfidf_march_dense, embeddings_march))\n",
    "\n",
    "# Update the 'embedding' column in the original dataframes\n",
    "df_jan['embedding'] = list(combined_jan)\n",
    "df_feb['embedding'] = list(combined_feb)\n",
    "df_march['embedding'] = list(combined_march)\n",
    "\n",
    "# Verify the new column format\n",
    "print(\"New January Embedding Shape:\", np.array(df_jan['embedding'].tolist()).shape)\n",
    "print(\"New February Embedding Shape:\", np.array(df_feb['embedding'].tolist()).shape)\n",
    "print(\"New March Embedding Shape:\", np.array(df_march['embedding'].tolist()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Class Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.32      0.38      0.35       123\n",
      "           0       0.83      0.76      0.79       542\n",
      "           1       0.12      0.17      0.14        48\n",
      "\n",
      "    accuracy                           0.66       713\n",
      "   macro avg       0.42      0.44      0.43       713\n",
      "weighted avg       0.69      0.66      0.67       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data (January and February combined)\n",
    "X_train_multi = np.vstack(pd.concat([df_jan, df_feb])['embedding'].values)\n",
    "y_train_multi = pd.concat([df_jan, df_feb])['label']\n",
    "\n",
    "# Prepare test data (March)\n",
    "X_test_multi = np.vstack(df_march['embedding'].values)\n",
    "y_test_multi = df_march['label']\n",
    "\n",
    "# Train multi-class logistic regression model\n",
    "multi_clf = LogisticRegression(max_iter=1000)\n",
    "multi_clf.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_multi = multi_clf.predict(X_test_multi)\n",
    "print(\"Multi-Class Classification Report:\\n\", classification_report(y_test_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.79      0.77       123\n",
      "           1       0.40      0.35      0.37        48\n",
      "\n",
      "    accuracy                           0.67       171\n",
      "   macro avg       0.58      0.57      0.57       171\n",
      "weighted avg       0.66      0.67      0.66       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter training data for binary classification\n",
    "df_train_binary = pd.concat([df_jan, df_feb])[pd.concat([df_jan, df_feb])['label'] != 0]\n",
    "df_march_binary = df_march[df_march['label'] != 0]\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train_binary = np.vstack(df_train_binary['embedding'].values)\n",
    "y_train_binary = df_train_binary['label']\n",
    "\n",
    "X_test_binary = np.vstack(df_march_binary['embedding'].values)\n",
    "y_test_binary = df_march_binary['label']\n",
    "\n",
    "# Train binary logistic regression model\n",
    "binary_clf = LogisticRegression(max_iter=1000)\n",
    "binary_clf.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_binary = binary_clf.predict(X_test_binary)\n",
    "print(\"Binary Classification Report:\\n\", classification_report(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report saved to: results/tf-idf_experiment_v1/metrics\\classification_report_multi.txt\n",
      "Confusion matrix saved to: results/tf-idf_experiment_v1/metrics\\confusion_matrix_multi.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_dir = \"results/tf-idf_experiment_v1/metrics\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "cm_multi = confusion_matrix(y_test_multi, y_pred_multi)\n",
    "class_report = classification_report(y_test_multi, y_pred_multi, target_names=[\"-1\", \"0\", \"1\"])\n",
    "\n",
    "report_path = os.path.join(results_dir, \"classification_report_multi.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(class_report)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_multi, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"-1\", \"0\", \"1\"], yticklabels=[\"-1\", \"0\", \"1\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Multi-Class Confusion Matrix\")\n",
    "\n",
    "conf_matrix_path = os.path.join(results_dir, \"confusion_matrix_multi.png\")\n",
    "plt.savefig(conf_matrix_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Classification report saved to: {report_path}\")\n",
    "print(f\"Confusion matrix saved to: {conf_matrix_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification report saved to: results/tf-idf_experiment_v1/metrics\\classification_report_binary.txt\n",
      "Binary confusion matrix saved to: results/tf-idf_experiment_v1/metrics\\confusion_matrix_binary.png\n"
     ]
    }
   ],
   "source": [
    "cm_binary = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "class_report_binary = classification_report(y_test_binary, y_pred_binary, target_names=[\"-1\", \"1\"])\n",
    "\n",
    "report_path_binary = os.path.join(results_dir, \"classification_report_binary.txt\")\n",
    "with open(report_path_binary, \"w\") as f:\n",
    "    f.write(\"Classification Report (Binary):\\n\")\n",
    "    f.write(class_report_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_binary, annot=True, fmt=\"d\", cmap=\"Greens\", \n",
    "            xticklabels=[\"-1\", \"1\"], yticklabels=[\"-1\", \"1\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Binary Confusion Matrix\")\n",
    "\n",
    "conf_matrix_path_binary = os.path.join(results_dir, \"confusion_matrix_binary.png\")\n",
    "plt.savefig(conf_matrix_path_binary)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Binary classification report saved to: {report_path_binary}\")\n",
    "print(f\"Binary confusion matrix saved to: {conf_matrix_path_binary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
