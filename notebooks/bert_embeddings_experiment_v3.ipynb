{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\fx-news-alpha\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Text Processing\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Model Saving\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/labeled_january_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/labeled_january_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     df_jan \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[0;32m      5\u001b[0m file_path2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/labeled_february_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\scaro\\Downloads\\fx-news-alpha\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/labeled_january_data.csv'"
     ]
    }
   ],
   "source": [
    "file_path1 = \"data/processed/labeled_january_data.csv\"\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)\n",
    "\n",
    "file_path2 = \"data/processed/labeled_february_data.csv\"\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)\n",
    "\n",
    "file_path3 = \"data/processed/labeled_march_data.csv\"\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_march = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    # Replace monetary values (e.g., \"R$ 15 bilhões\" → \"15B\")\n",
    "    text = re.sub(r\"R\\$ ?([\\d.,]+) bilhões\", r\"\\1B\", text)\n",
    "    text = re.sub(r\"R\\$ ?([\\d.,]+) milhões\", r\"\\1M\", text)\n",
    "    # Standardize percentages (e.g., \"0.5 pp\" → \"0.5%\")\n",
    "    text = re.sub(r\"([\\d.,]+) pp\", r\"\\1%\", text)\n",
    "    # Standardize plain numbers (e.g., \"15,000\" → \"15000\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    # Remove dates and times\n",
    "    text = re.sub(r\"\\b\\d{1,2}[hH]\\d{2}\\b\", \"\", text)  # Times like \"17h20\"\n",
    "    text = re.sub(r\"\\b\\d{1,2}ª[Ff]\\b\", \"\", text)       # Ordinals like \"2ªF\"\n",
    "    # Remove redundant words\n",
    "    text = re.sub(r\"MAIS AGENDA|LÁ FORA|A BOLSA ESTÁ CARA\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dictionary for expanding acronyms\n",
    "acronyms = {\n",
    "    \"Selic\": \"Sistema Especial de Liquidação e de Custódia\",\n",
    "    \"PIB\": \"Produto Interno Bruto\",\n",
    "    \"CDI\": \"Certificado de Depósito Interbancário\",\n",
    "    \"LPRs\": \"Loan Prime Rates\",\n",
    "    \"Ibovespa\": \"Índice Bovespa\",\n",
    "    \"BB\": \"Banco do Brasil\",\n",
    "    \"BC\": \"Banco Central\",\n",
    "    \"FGTS\": \"Fundo de Garantia do Tempo de Serviço\",\n",
    "    \"STF\": \"Supremo Tribunal Federal\",\n",
    "    \"CPI\": \"Índice de Preços ao Consumidor\",\n",
    "    \"MP\": \"Medida Provisória\",\n",
    "    \"EUA\": \"Estados Unidos\",\n",
    "    \"ONU\": \"Organização das Nações Unidas\",\n",
    "    \"FGV\": \"Fundação Getúlio Vargas\",\n",
    "    \"IBGE\": \"Instituto Brasileiro de Geografia e Estatística\",\n",
    "    \"BNDES\": \"Banco Nacional de Desenvolvimento Econômico e Social\",\n",
    "    \"IPCA\": \"Índice Nacional de Preços ao Consumidor Amplo\",\n",
    "    \"DI\": \"Depósito Interfinanceiro\",\n",
    "    \"IR\": \"Imposto de Renda\",\n",
    "    \"OI\": \"Operadora Oi\",\n",
    "    \"CV\": \"Câmara de Vereadores\"\n",
    "}\n",
    "\n",
    "noisy_acronyms = {\"ROMI\", \"ENEVA\", \"LIGHT\", \"DA\"}\n",
    "\n",
    "def expand_acronyms(text, acronym_dict):\n",
    "    for acronym, full_form in acronym_dict.items():\n",
    "        text = re.sub(rf'\\b{re.escape(acronym)}\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_noisy_acronyms(text, noisy_set):\n",
    "    return re.sub(r'\\b(?:' + '|'.join(noisy_set) + r')\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Portuguese spaCy model\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = normalize_numbers(text)         \n",
    "    text = expand_acronyms(text, acronyms)\n",
    "    text = remove_noisy_acronyms(text, noisy_acronyms)\n",
    "    text = lemmatize_text(text)          \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df_jan['cleaned_article'] = df_jan['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")\n",
    "df_feb['cleaned_article'] = df_feb['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")\n",
    "df_march['cleaned_article'] = df_march['article'].apply(\n",
    "    lambda x: pd.Series(preprocess_text(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_jan['label'].value_counts())\n",
    "sns.countplot(x='label', data=df_jan)\n",
    "plt.title('Label Distribution in January Dataset')\n",
    "plt.savefig(\"results/bert_embeddings_experiment_v1/figures/label_distribution_january.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feb['label'].value_counts())\n",
    "sns.countplot(x='label', data=df_feb)\n",
    "plt.title('Label Distribution in February Dataset')\n",
    "plt.savefig(\"results/bert_embeddings_experiment_v1/figures/label_distribution_february.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_march['label'].value_counts())\n",
    "sns.countplot(x='label', data=df_march)\n",
    "plt.title('Label Distribution in March Dataset')\n",
    "plt.savefig(\"results/bert_embeddings_experiment_v1/figures/label_distribution_march.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTimbau tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    # tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # pass inputs through model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract [CLS] token embedding (shape: [batch_size, hidden_size])\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token is the first token\n",
    "    return cls_embedding.squeeze(0).numpy()  # convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan['embedding'] = df_jan['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "df_feb['embedding'] = df_feb['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "df_march['embedding'] = df_march['cleaned_article'].apply(lambda x: get_bert_embedding(x, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def knn_binary_classification(df_train, df_test, k=5):\n",
    "    df_train_binary = df_train[df_train['label'] != 0]\n",
    "    df_test_binary = df_test[df_test['label'] != 0]\n",
    "    \n",
    "    X_train = np.vstack(df_train_binary['embedding'].values)\n",
    "    y_train = df_train_binary['label']\n",
    "    X_test = np.vstack(df_test_binary['embedding'].values)\n",
    "    y_test = df_test_binary['label']\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"Binary Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[-1, 1])\n",
    "    plot_confusion_matrix(cm, labels=[-1, 1], title=\"Confusion Matrix: Binary Classification\")\n",
    "\n",
    "def knn_multi_classification(df_train, df_test, k=5):\n",
    "    X_train = np.vstack(df_train['embedding'].values)\n",
    "    y_train = df_train['label']\n",
    "    X_test = np.vstack(df_test['embedding'].values)\n",
    "    y_test = df_test['label']\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"Multi-class Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[-1, 0, 1])\n",
    "    plot_confusion_matrix(cm, labels=[-1, 0, 1], title=\"Confusion Matrix: Multi-class Classification\")\n",
    "\n",
    "df_train = pd.concat([df_jan, df_feb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Binary Classification (KNN):\")\n",
    "knn_binary_classification(df_train, df_march, k=5)\n",
    "\n",
    "print(\"\\nMulti-class Classification (KNN):\")\n",
    "knn_multi_classification(df_train, df_march, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
